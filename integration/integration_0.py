# -*- coding: utf-8 -*-
"""Integration.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nfDz00Ql3uC-PIqCcc_umk5DRM2RMhGO
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf 
device_name = tf.test.gpu_device_name() 
if device_name != '/device:GPU:0': 
  raise SystemError('GPU device not found') 
print('Found GPU at: {}'.format(device_name))

#!wget http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/mono/OpenSubtitles.raw.en.gz
#import os 
#!gunzip -c download.php?f=OpenSubtitles%2Fv2018%2Fmono%2FOpenSubtitles.raw.en.gz >/tmp/lines 
##os.mkdir ('/tmp/lines')
#!split -a 3 -l 100000 download.php?f=OpenSubtitles%2Fv2018%2Fmono%2FOpenSubtitles.raw.en.gz /tmp/lines-
##BUCKET="your-bucket"  
##gsutil -m cp -r lines gs://${BUCKET?}/opensubtitles/raw/

! wget https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2018/mono/en.txt.gz

! apt install gunzip
! gunzip en.txt.gz
#gunzip command is used to compress or expand a file or a list of files
#It accepts all the files having extension as .gz, .z, _z, -gz, -z , .Z, .taz or.tgz 
#and replace the compressed file with the original file by default. The files after uncompression retain its actual extension.
#The files after uncompression retain its actual extension.

!tail en.txt
#Refrence : https://www.geeksforgeeks.org/tail-command-linux-examples/
#It is the complementary of head command.
#The tail command, as the name implies, print the last N number of data of the given input. 
#By default it prints the last 10 lines of the specified files.

!tail -n 3 en.txt
#Refrence : https://www.geeksforgeeks.org/tail-command-linux-examples/
#Prints the last ‘num’ lines instead of last 10 lines. num is mandatory to be specified in command otherwise it displays an error. 
#This command can also be written as without symbolizing ‘n’ character but ‘-‘ sign is mandatory.

import os
os.mkdir('/content/lines')

#! split -a 3 -l 100000 en.txt lines/lines-
! split -a 3 -l 100000 --verbose en.txt /content/lines-
#Reference: https://kb.iu.edu/d/afar#:~:text=The%20split%20command%20will%20give,prefix%2C%20most%20systems%20use%20x%20.

def line_cleaner(line): 
  line=line.decode("utf-8")
#  print(type(line))
  clean_line=line.strip(" -.!?\n")
  clean_line=clean_line.lower()
  return clean_line

line_cleaner(b'- Hello?\n')

lines=[]
i=0
file= open('/content/lines-aaa','rb')
for line in file:
  #print(line)
  line=line_cleaner(line)
  i=i+1
  if(i>=50000):
    break
  lines.append(line)
print(i,lines)

# line_cleaner is a function which brings all the characters to lowercase and removes some of the punctuations
def line_cleaner2(line):
    line=line.lower()
    temp=""
    for char in line:
#       print(char)
        if char!= "," and char!= "!" and char!= "?" and char!= "." and char!= "-":
         if char!=',':
            temp+=char
    return temp

#example
line_cleaner2("Hey! How ya doin *wink* ?")

#Referense: https://www.tutorialspoint.com/python/python_reg_expressions.htm#:~:text=A%20regular%20expression%20is%20a,syntax%20held%20in%20a%20pattern.&text=The%20Python%20module%20re%20provides,like%20regular%20expressions%20in%20Python.
import re

def _should_skip(line, min_length, max_length):
    """Whether a line should be skipped depending on the length."""
    return len(line) < min_length or len(line) > max_length

def create_example(previous_lines, line, file_id):
    """Creates examples with multi-line context
    The examples will include:
        file_id: the name of the file where these lines were obtained.
        response: the current line text
        context: the previous line text
        context/0: 2 lines before
        context/1: 3 lines before, etc.
    """
    example = {
        'file_id': file_id,
        'context': previous_lines[-1],
        'response': line,
    }
    example['file_id'] = file_id
    example['context'] = previous_lines[-1]

    extra_contexts = previous_lines[:-1]
    example.update({
        'context/{}'.format(i): context
        for i, context in enumerate(extra_contexts[::-1])
    })

    return example

def _preprocess_line(line):
    line = line.decode("utf-8")

    # Remove the first word if it is followed by colon (speaker names)
    # NOTE: this wont work if the speaker's name has more than one word
    #re. sub() function is used to replace occurrences of a particular sub-string with another sub-string.
    line = re.sub('(?:^|(?:[.!?]\\s))(\\w+):', "", line)

    # Remove anything between brackets (corresponds to acoustic events).
    line = re.sub("[\\[(](.*?)[\\])]", "", line)

    # Strip blanks hyphens and line breaks
    line = line.strip(" -\n")

    return line

def _create_examples_from_file(file_name, min_length=0, max_length=20,
                               num_extra_contexts=5):

    previous_lines = []
    with open(file_name, 'rb') as f:
      for line in f :
        line = _preprocess_line(line)
        if not line:
            continue

        should_skip = _should_skip(
            line,
            min_length=min_length,
            max_length=max_length)

        if previous_lines:
            should_skip |= _should_skip(
                previous_lines[-1],
                min_length=min_length,
                max_length=max_length)

            if not should_skip:
                yield create_example(previous_lines, line, file_name)

        previous_lines.append(line)
        if len(previous_lines) > num_extra_contexts + 1:
            del previous_lines[0]

def _features_to_serialized_tf_example(features):
    """Convert a string dict to a serialized TF example.
    The dictionary maps feature names (strings) to feature values (strings).
    """
    example = tf.train.Example()
    for feature_name, feature_value in features.items():
        example.features.feature[feature_name].bytes_list.value.append(
            feature_value.encode("utf-8"))
    return example.SerializeToString()

def _shuffle_examples(examples):
    examples |= ("add random key" >> beam.Map(
        lambda example: (uuid.uuid4(), example)))
    examples |= ("group by key" >> beam.GroupByKey())
    examples |= ("get shuffled values" >> beam.FlatMap(lambda t: t[1]))
    return examples

example = _create_examples_from_file(file_name='/content/lines-gmz')

count = 0
for i in example:
  count += 1
print('Found '+ str(count*5) + ' examples')
print(i)

test = _create_examples_from_file(file_name='/content/lines-gmz')
for x in test:
  print(x)

test = _create_examples_from_file(file_name='/content/lines-gmz')
for x in test:
  #print(x['context/0'])
   print(x['context/4'])

test = _create_examples_from_file(file_name='/content/lines-gmz')
inputs = []
responses = []
i = 0

for t in test:

  response= t['response']
  input_1 = t['context']
  input_2 = t['context/0']
  input_3 = t['context/1']
  input_4 = t['context/2']
  input_5 = t['context/3']
  
  inputs.append(line_cleaner2(input_1))
  inputs.append(line_cleaner2(input_2))
  inputs.append(line_cleaner2(input_3))
  inputs.append(line_cleaner2(input_4))
  inputs.append(line_cleaner2(input_5))
  #responses.append('startsent' + ' ' +line_cleaner2(response)+ ' ' + 'endsent')
    

  for j in range(5):
    responses.append('startsent' + ' ' +line_cleaner2(response)+ ' ' + 'endsent')
    
  i += 5

print(len(inputs))
print(len(responses))

print(inputs[102219], responses[102219])

from tensorflow.keras.preprocessing.text import Tokenizer 
from tensorflow.keras.preprocessing.sequence import pad_sequences

oov_token = "<OOV>"
max_length = 25

tokenizer = Tokenizer(oov_token=oov_token)
tokenizer.fit_on_texts(inputs)
tokenizer.fit_on_texts(responses)

word_index = tokenizer.word_index
word_index['startsent'] = 0
word_index['endsent'] = len(word_index)+1
index_word = {word_index[word]:word for word in word_index}
print(index_word)

vocab_size = len(word_index) + 1
input_seq = tokenizer.texts_to_sequences(inputs)
response_seq = tokenizer.texts_to_sequences(responses)
input_seq_pad = pad_sequences(input_seq, maxlen = max_length ,padding = 'post', truncating = 'post')
response_seq_pad = pad_sequences(response_seq, maxlen = max_length, padding = 'post', truncating = 'post')

print(len(word_index))
print(inputs[0], responses[0])
print(input_seq_pad[0], response_seq_pad[0])
print(len(input_seq_pad))

def preprocess(text_list):
  inputs = []
  for tmp in text_list:
    inputs.append(line_cleaner2(tmp))
  input_seq = tokenizer.texts_to_sequences(inputs)
  input_seq_pad = pad_sequences(input_seq, maxlen = max_length ,padding = 'post', truncating = 'post')
  return input_seq_pad

"""loading pre-trained word embeddings (GloVe embeddings)into a frozen Keras Embedding layer, and using it to train a attention model

Reference1 : https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html

Reference2 : https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py

GloVe embedding data can be found at : http://nlp.stanford.edu/data/glove.6B.zip
(source page: http://nlp.stanford.edu/projects/glove/)
compute an index mapping words to known embeddings, by parsing the data dump of pre-trained embeddings:
first, build index mapping words in the embeddings set to their embedding vector
"""

!wget http://nlp.stanford.edu/data/glove.6B.zip 
#download the zip file

!unzip glove.6B.zip 
#unzip it

#Get the exact path of where the embedding vectors are extracted 
!ls
!pwd

"""Preparing the Embedding layer"""

# compute an index mapping words to known embeddings, by parsing the data dump of pre-trained embeddings:
#Index the vectors

import numpy as np
import os

print('Indexing word vectors.')
GLOVE_DIR = './'

embeddings_index = {} #initialize dictionary
f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('Found %s word vectors.' % len(embeddings_index))

#Fuse with google - drive
#Referense : https://stackoverflow.com/questions/50060241/how-to-use-glove-word-embeddings-file-on-google-colaboratory
!pip install --upgrade pip
!pip install -U -q pydrive
!apt-get install -y -qq software-properties-common python-software-properties module-init-tools
!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null
!apt-get update -qq 2>&1 > /dev/null

!apt-get -y install -qq google-drive-ocamlfuse fuse

from google.colab import auth
auth.authenticate_user()
# Generate creds for the Drive FUSE library.
from oauth2client.client import GoogleCredentials
creds = GoogleCredentials.get_application_default()
import getpass
!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL
vcode = getpass.getpass()
!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}

!mkdir -p drive
!google-drive-ocamlfuse drive

#Save the indexed vectors to google drive for re-use

import pickle
pickle.dump({'embeddings_index' : embeddings_index } , open('/content/glove.6B.100d.txt', 'wb'))

#from google.colab import files
#files.upload()

#compute our embedding matrix

embedding_dim = 100
embeddings_matrix = np.zeros((vocab_size+1, embedding_dim))
#create an array of zeros with num_words rows and embedding_dim columns

for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embeddings_matrix[i] = embedding_vector

print(len(embeddings_matrix))
embeddings_matrix
#Printing the embedding matrix we just formed and it's length

print(len(embeddings_matrix))
embeddings_matrix[27]

print(len(input_seq_pad))
input_seq_pad

print(len(response_seq_pad))
response_seq_pad

#Preparing training dataset
X_train = []
y_train = []
for j in range(102219):
    X_train.append(input_seq_pad[j])
    y_train.append(response_seq_pad[j])

X_train = np.array(X_train)
y_train = np.array(y_train)

print(embedding_dim)
print(X_train.shape)
print(y_train.shape)

dataset = (inputs, responses)

import os
import pickle
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding, GRU, Dense 

max_sequence_len = 25
batch_size = 64
train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
train_dataset = train_dataset.cache()
train_dataset = train_dataset.shuffle(1024)
train_dataset = train_dataset.batch(batch_size, drop_remainder=True)
train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)

"""**Implementation with Gensim**

Here, we are going to use LDA (Latent Dirichlet Allocation) to extract the naturally discussed topics from dataset.
"""

from sklearn.datasets import fetch_20newsgroups 
newsgroups_train = fetch_20newsgroups(subset='train')

newsgroups_train.data[:2]

#We need Stopwords from NLTK and English model from Scapy. Both can be downloaded as follows
import nltk
import spacy
spacy.load('en')
from spacy.lang.en import English
parser = English()
nltk.download('stopwords')
#nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])

!pip install pyLDAvis
#Importing Necessary Packages

import re
import numpy as np
import pandas as pd
from pprint import pprint
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
import spacy
import pyLDAvis
import pyLDAvis.gensim
import matplotlib.pyplot as plt

from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use'])

"""Clean up the Text

Now, with the help of Gensim’s simple_preprocess() we need to tokenise each sentence into a list of words. We should also remove the punctuations and unnecessary characters. In order to do this, we will create a function named sent_to_words() −
"""

data = newsgroups_train.data
data = [re.sub('\S*@\S*\s?', '', sent) for sent in data]
data = [re.sub('\s+', ' ', sent) for sent in data]
data = [re.sub("\'", "", sent) for sent in data]

def sent_to_words(sentences):
   for sentence in sentences:
      yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))
data_words = list(sent_to_words(data))

print(data_words[:4]) #it will print the data after prepared for stopwords
len(data_words)

def remove_stopwords(texts):
   return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]

data_words_no_stopw = list(remove_stopwords(data_words))

print(data_words_no_stopw[:4])
len(data_words_no_stopw)

#nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])
#def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
#   texts_out = []
#   for sent in texts:
#      doc = nlp(" ".join(sent))
#      texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
#   return texts_out

nltk.download('wordnet')

from nltk.stem import WordNetLemmatizer
lemm = WordNetLemmatizer()
print("The lemmatized form of leaves is: {}".format(lemm.lemmatize("leaves")))

def lemmatize(texts):
   return [[lemm.lemmatize(word) for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]

data_lem=lemmatize(data_words_no_stopw)

data_lem[:4]

id2word = corpora.Dictionary(data_lem)

from gensim import corpora
dictionary = corpora.Dictionary(data_lem)
corpus = [dictionary.doc2bow(text) for text in data_lem]
import pickle
pickle.dump(corpus, open('corpus.pkl', 'wb'))
dictionary.save('dictionary.gensim')

import gensim
NUM_TOPICS = 25
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)
ldamodel.save('model5.gensim')
topics = ldamodel.print_topics(num_words=7)
for topic in topics:
    print(topic)

pprint(ldamodel.print_topics())
doc_lda = ldamodel[corpus]

print('\nPerplexity: ', ldamodel.log_perplexity(corpus))

new_text = 'How many species of animals are there in Russia and in the US, and where are the biggest oceans?'
tokens=lemmatize(new_text)
ldamodel[[dictionary.doc2bow(text) for text in tokens]]

from nltk.stem import WordNetLemmatizer, PorterStemmer
from nltk.stem.porter import *
stemmer = PorterStemmer()
def lemmatize_stemming(text):
    x = WordNetLemmatizer().lemmatize(text, pos='v')
    return stemmer.stem(x)
def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
            result.append(lemmatize_stemming(token))
    return result

import random

def extract_topic(input):

  c = 0
  bow_vector = dictionary.doc2bow(preprocess(input))
  for index, score in sorted(ldamodel[bow_vector], key=lambda tup: -1*tup[1]):
      wp = ldamodel.show_topic(index)
      topic_keywords = ", ".join([word for word, prop in wp])
      if c == random.randint(0,30):
         break
      c += 1
  return topic_keywords

def get_topic(input):

    input_topic = extract_topic(input)
    input_seq = tokenizer.texts_to_sequences([input_topic])
    input_seq_pad = pad_sequences(input_seq, maxlen = max_length ,padding = 'post', truncating = 'post')
    return input_seq_pad[0]

get_topic('How many species of animals are there in Russia and in the US, and where are the biggest oceans?')



"""**Encoder, Decoder and Attention** models using tf.keras model subclassing"""

class Encoder(tf.keras.Model):
    def __init__(self, hidden_size=1024, max_sequence_len=25, batch_size=batch_size, embedding_dim=100, vocab_size=vocab_size+1):
        super(Encoder, self).__init__()
        self.embedding_dim = embedding_dim
        self.vocab_size = vocab_size
        self.max_sequence_len = max_sequence_len
        self.hidden_size = hidden_size
        self.batch_size = batch_size

        self.embedding_layer = Embedding(
            input_dim=self.vocab_size, output_dim=self.embedding_dim, weights=[embeddings_matrix], trainable=False)
        self.GRU_1 = GRU(units=hidden_size, return_sequences=True,recurrent_initializer='glorot_uniform')
        self.GRU_2 = GRU(units=hidden_size,
                         return_sequences=True, return_state=True,recurrent_initializer='glorot_uniform')

    def initial_hidden_state(self):
        return tf.zeros(shape=(self.batch_size, self.hidden_size))

    def call(self, x, initial_state, training=False):
        x = self.embedding_layer(x)
        x = self.GRU_1(x, initial_state=initial_state)
        x, hidden_state = self.GRU_2(x)
        return x, hidden_state

class Attention(tf.keras.Model):
    def __init__(self, hidden_size=256):
        super(Attention, self).__init__()
        self.fc1 = Dense(units=hidden_size)
        self.fc2 = Dense(units=hidden_size)
        self.fc3 = Dense(units=1)

    def call(self, encoder_output, hidden_state, training=False):
        '''hidden_state : h(t-1)'''
        y_hidden_state = tf.expand_dims(hidden_state, axis=1)
        y_hidden_state = self.fc1(y_hidden_state)
        y_enc_out = self.fc2(encoder_output)

        y = tf.keras.backend.tanh(y_enc_out + y_hidden_state)
        attention_score = self.fc3(y)
        attention_weights = tf.keras.backend.softmax(attention_score, axis=1)

        context_vector = tf.multiply(encoder_output, attention_weights)
        context_vector = tf.reduce_sum(context_vector, axis=1)

        return context_vector, attention_weights

class Decoder(tf.keras.Model):
    def __init__(self, hidden_size=1024, max_sequence_len=25, batch_size=batch_size, embedding_dim=100, vocab_size=vocab_size+1):
        super(Decoder, self).__init__()
        self.embedding_dim = embedding_dim
        self.vocab_size = vocab_size
        self.max_sequence_len = max_sequence_len
        self.hidden_size = hidden_size
        self.batch_size = batch_size
    
        self.embedding_layer = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, weights=[embeddings_matrix], trainable=False)
        self.GRU = GRU(units=hidden_size,
                       return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')
        self.attention = Attention(hidden_size=self.hidden_size)
        self.fc = Dense(units=self.vocab_size)

    def initial_hidden_state(self):
        return tf.zeros(shape=(self.batch_size, self.hidden_size))

    def call(self, x, encoder_output, hidden_state, training=False):
        x = self.embedding_layer(x)
        context_vector, attention_weights = self.attention(
            encoder_output, hidden_state, training=training)
        contect_vector = tf.expand_dims(context_vector, axis=1)
        x = tf.concat([x, contect_vector], axis=-1)
        x, curr_hidden_state = self.GRU(x)
        x = tf.reshape(x, shape=[self.batch_size, -1])
        x = self.fc(x)
        return x, curr_hidden_state, attention_weights

#Defining training loop, loss function and optimizer

loss_object = tf.losses.SparseCategoricalCrossentropy(from_logits=True)
optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)
train_accuracy = tf.metrics.SparseCategoricalAccuracy()

def loss_function(y_true, y_pred):
    loss = loss_object(y_true, y_pred)
    mask = 1 - tf.cast(tf.equal(y_true, 0), 'float32')
    return tf.reduce_mean(loss * mask)

@tf.function()  #https://www.tensorflow.org/api_docs/python/tf/function
def training_step(inputs, responses):    
    with tf.GradientTape() as Tape:
        encoder_init_state = encoder.initial_hidden_state()
        encoder_output, encoder_hidden_state = encoder(inputs, encoder_init_state, training=True)
        decoder_hidden = encoder_hidden_state
        loss = 0
        acc = []
        current_word = tf.expand_dims(responses[:, 0], axis=1)
        for word_idx in range(1, max_sequence_len):
            next_word = responses[:, word_idx]
            logits, decoder_hidden, attention_weights = decoder(current_word, encoder_output, decoder_hidden)
            loss += loss_function(next_word, logits)
            acc.append(train_accuracy(next_word, logits))
            current_word = tf.expand_dims(next_word, axis=1)
    variables = encoder.trainable_variables + decoder.trainable_variables
    gradients = Tape.gradient(loss, variables)
    optimizer.apply_gradients(zip(gradients, variables))
    return loss, tf.reduce_mean(acc)

encoder = Encoder()
decoder = Decoder()

checkpoint_dir = 'training'
checkpoint_prefix = 'training/ckpt'
checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)

#Training the model

epochs = 7
NUM_SAMPLES = len(X_train)
num_steps = NUM_SAMPLES // batch_size

for epoch in range(1, epochs + 1):
    print(f'Epoch {epoch}/{epochs}')
    ep_loss = []
    ep_acc = []
    progbar = tf.keras.utils.Progbar(target=num_steps, stateful_metrics=[
                                     'curr_loss', 'curr_accuracy'], unit_name='batch')

    for step, example in enumerate(train_dataset):
        inputs = example[0] 
        responses = example[1]
        loss, acc = training_step(inputs, responses)
        loss /= responses.shape[1]
        ep_loss.append(loss)
        ep_acc.append(acc)
        progbar.update(
            step + 1, values=[('curr_loss', loss), ('curr_accuracy', acc)])

    print(f'Metrics after epoch {epoch} : Loss => {np.mean(ep_loss):.3f} | Accuracy => {np.mean(ep_acc):.3f}')

#Inference function

def create_response(sentence):
    sentence = preprocess([sentence])
    enc_init = tf.zeros(shape=[1, 1024])
    enc_out, enc_hidden = encoder(sentence, enc_init)
    decoder.batch_size = 1
    tokenizer.index_word[0] = ''
    decoded = []
    att = []
    current_word = tf.expand_dims([word_index['startsent']], axis=0) 
    decoder_hidden = enc_hidden
    for word_idx in range(1, max_sequence_len):
        logits, decoder_hidden, attention_weights = decoder(current_word, enc_out, decoder_hidden)
        decoded_idx = np.argmax(logits)
        if index_word[decoded_idx] == 'endsent':
            break
        decoded.append(tokenizer.index_word[decoded_idx])
        att.append(attention_weights.numpy().squeeze())
        current_word = tf.expand_dims([decoded_idx], axis=0)
    return ' '.join(decoded), att

sentences = ['nice to meet you',
             'i like cake',
             'sports are fun',
             'It is going to be hard', 
             'But i guess that is how life is',
             'its getting late',
             '4 in the morning already',
             'you feeling sleepy?', 
             'this code took way to long to run',
             'you are a mean bot',
             'are you real?',
             'have a nice day',
             'i have started to like this']
                          

for inp_sentence in sentences:
    inp_array = inp_sentence.split()
    inp_len = len(inp_sentence.split())
    trans_sentence, attention_weights = create_response(inp_sentence)
    trans_array = trans_sentence.split()
    trans_len = len(trans_array)
    print('INPUT : ', inp_sentence)
    print('RESPONSE : ', trans_sentence)
    print('-'*30)

sentences_ = ['how are you ?',
             'do you miss college?',
             'It is going to be hard', 
             'i like this',
             'how are you doing?',
             'help', 
             'are you sad?']

for inp_sentence in sentences_:
    inp_array = inp_sentence.split()
    inp_len = len(inp_sentence.split())
    trans_sentence, attention_weights = create_response(inp_sentence)
    trans_array = trans_sentence.split()
    trans_len = len(trans_array)
    print('INPUT : ', inp_sentence)
    print('RESPONSE : ', trans_sentence)
    print('-'*30)

epochs = 5
NUM_SAMPLES = len(X_train)
num_steps = NUM_SAMPLES // batch_size

for epoch in range(1, epochs + 1):
    print(f'Epoch {epoch}/{epochs}')
    ep_loss = []
    ep_acc = []
    progbar = tf.keras.utils.Progbar(target=num_steps, stateful_metrics=[
                                     'curr_loss', 'curr_accuracy'], unit_name='batch')

    for step, example in enumerate(train_dataset):
        inputs = example[0]
        responses = example[1]
        loss, acc = training_step(inputs, responses)
        loss /= responses.shape[1]
        ep_loss.append(loss)
        ep_acc.append(acc)
        progbar.update(
            step + 1, values=[('curr_loss', loss), ('curr_accuracy', acc)])
    if epoch % 2 == 0:
        checkpoint.save(file_prefix=checkpoint_prefix)
    print(f'Metrics after epoch {epoch} : Loss => {np.mean(ep_loss):.3f} | Accuracy => {np.mean(ep_acc):.3f}')

checkpoint.save(file_prefix=checkpoint_prefix)

sentences2 = ['nice to meet you',
             'i like cake',
             'sports are fun',
             'It is going to be hard', 
             'But i guess that is how life is',
             'its getting late',
             '4 in the morning already',
             'you feeling sleepy?', 
             'this code took way to long to run',
             'you are a mean bot',
             'are you real?',
             'have a nice day',
             'i have started to like this']
                          

for inp_sentence in sentences2:
    inp_array = inp_sentence.split()
    inp_len = len(inp_sentence.split())
    trans_sentence, attention_weights = create_response(inp_sentence)
    trans_array = trans_sentence.split()
    trans_len = len(trans_array)
    print('INPUT : ', inp_sentence)
    print('RESPONSE : ', trans_sentence)
    print('-'*30)

sentences3 = ['nice to meet you',
             'i like cake',
              'i love you',
              'i am very bored ',
              'let us go out to eat',
              'i miss paneer mayo',
              'Please help me',
              'Please stop', 
             'sports are fun',
             'It is going to be hard', 
             'But i guess that is how life is',
             'its getting late',
             '4 in the morning already',
             'you feeling sleepy?', 
             'this code took way to long to run',
             'you are a mean bot',
             'are you real?',
             'have a nice day',
             'i have started to like this']
                          

for inp_sentence in sentences3:
    inp_array = inp_sentence.split()
    inp_len = len(inp_sentence.split())
    trans_sentence, attention_weights = create_response(inp_sentence)
    trans_array = trans_sentence.split()
    trans_len = len(trans_array)
    print('INPUT : ', inp_sentence)
    print('RESPONSE : ', trans_sentence)
    print('-'*30)

"""**Speech To Text**"""

!pip install ffmpeg-python

"""
To write this piece of code I took inspiration/code from a lot of places.
It was late night, so I'm not sure how much I created or just copied o.O
Here are some of the possible references:
https://blog.addpipe.com/recording-audio-in-the-browser-using-pure-html5-and-minimal-javascript/
https://stackoverflow.com/a/18650249
https://hacks.mozilla.org/2014/06/easy-audio-capture-with-the-mediarecorder-api/
https://air.ghost.io/recording-to-an-audio-file-using-html5-and-js/
https://stackoverflow.com/a/49019356
"""
from IPython.display import HTML, Audio
from google.colab.output import eval_js
from base64 import b64decode
import numpy as np
from scipy.io.wavfile import read as wav_read
import io
import ffmpeg

AUDIO_HTML = """
<script>
var my_div = document.createElement("DIV");
var my_p = document.createElement("P");
var my_btn = document.createElement("BUTTON");
var t = document.createTextNode("Press to start recording");

my_btn.appendChild(t);
//my_p.appendChild(my_btn);
my_div.appendChild(my_btn);
document.body.appendChild(my_div);

var base64data = 0;
var reader;
var recorder, gumStream;
var recordButton = my_btn;

var handleSuccess = function(stream) {
  gumStream = stream;
  var options = {
    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k
    mimeType : 'audio/webm;codecs=opus'
    //mimeType : 'audio/webm;codecs=pcm'
  };            
  //recorder = new MediaRecorder(stream, options);
  recorder = new MediaRecorder(stream);
  recorder.ondataavailable = function(e) {            
    var url = URL.createObjectURL(e.data);
    var preview = document.createElement('audio');
    preview.controls = true;
    preview.src = url;
    document.body.appendChild(preview);

    reader = new FileReader();
    reader.readAsDataURL(e.data); 
    reader.onloadend = function() {
      base64data = reader.result;
      //console.log("Inside FileReader:" + base64data);
    }
  };
  recorder.start();
  };

recordButton.innerText = "Recording... press to stop";

navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);


function toggleRecording() {
  if (recorder && recorder.state == "recording") {
      recorder.stop();
      gumStream.getAudioTracks()[0].stop();
      recordButton.innerText = "Saving the recording... pls wait!"
  }
}

// https://stackoverflow.com/a/951057
function sleep(ms) {
  return new Promise(resolve => setTimeout(resolve, ms));
}

var data = new Promise(resolve=>{
//recordButton.addEventListener("click", toggleRecording);
recordButton.onclick = ()=>{
toggleRecording()

sleep(2000).then(() => {
  // wait 2000ms for the data to be available...
  // ideally this should use something like await...
  //console.log("Inside data:" + base64data)
  resolve(base64data.toString())

});

}
});
      
</script>
"""

def get_audio():
  display(HTML(AUDIO_HTML))
  data = eval_js("data")
  binary = b64decode(data.split(',')[1])
  
  process = (ffmpeg
    .input('pipe:0')
    .output('pipe:1', format='wav')
    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)
  )
  output, err = process.communicate(input=binary)
  
  riff_chunk_size = len(output) - 8
  # Break up the chunk size into four bytes, held in b.
  q = riff_chunk_size
  b = []
  for i in range(4):
      q, r = divmod(q, 256)
      b.append(r)

  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.
  riff = output[:4] + bytes(b) + output[8:]

  sr, audio = wav_read(io.BytesIO(riff))

  return audio, sr

!pip3 install SpeechRecognition pydub

audio, sr = get_audio()
import scipy
scipy.io.wavfile.write('recording.wav', sr, audio)
filename = 'recording.wav'
import speech_recognition as sr
r = sr.Recognizer()
with sr.AudioFile(filename) as source:
    # listen for the data (load audio to memory)
    audio_data = r.record(source)
    # recognize (convert from speech to text)
    text = r.recognize_google(audio_data)
    print(text)

inp_sentence = text
inp_array = inp_sentence.split()
inp_len = len(inp_sentence.split())
trans_sentence, attention_weights = create_response(inp_sentence)
trans_array = trans_sentence.split()
trans_len = len(trans_array)
print('INPUT : ', inp_sentence)
print('RESPONSE : ', trans_sentence)
print('-'*30)

"""**Text To Speech**"""

!pip install gtts

!pip install IPython

!pip install pygame

import IPython

import os
from gtts import gTTS
import pyglet

text_to_read = trans_sentence
language = 'en'
slow_audio_speed = False
out_filename = 'my_file.mp3'

def reading_response():
    
    audio_created = gTTS(text=text_to_read, lang=language,slow=slow_audio_speed)
                        
    audio_created.save(out_filename)
        
    os.system(f'start {out_filename}')

reading_response()

IPython.display.Audio('/content/my_file.mp3')

#text='START'
#while(text is not 'stop'):
#    audio, sr = get_audio()
#    import scipy
#    scipy.io.wavfile.write('recording.wav', sr, audio)
#    filename = 'recording.wav'
#    import speech_recognition as sr
#    r = sr.Recognizer()
#    with sr.AudioFile(filename) as source:
#        # listen for the data (load audio to memory)
#        audio_data = r.record(source)
#        # recognize (convert from speech to text)
#        text = r.recognize_google(audio_data)
#        print('So you said:'+text)
#        #print(text)
#    trans_sentence, attention_weights = create_response(text)
#    text_to_read = trans_sentence
#    reading_response()
#    print('ConvoBot:'+trans_sentence)
#    IPython.display.Audio('/content/my_file.mp3')

audio, sr = get_audio()
import scipy
scipy.io.wavfile.write('recording.wav', sr, audio)
filename = 'recording.wav'
import speech_recognition as sr
r = sr.Recognizer()
with sr.AudioFile(filename) as source:
    # listen for the data (load audio to memory)
    audio_data = r.record(source)
    os.remove(filename) 
    # recognize (convert from speech to text)
    text = r.recognize_google(audio_data)
    print('So you said:'+text)
    #print(text)
trans_sentence, attention_weights = create_response(text)
text_to_read = trans_sentence  
reading_response() 
print('ConvoBot:'+trans_sentence) 
IPython.display.Audio('/content/my _file.mp3')