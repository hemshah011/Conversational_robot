{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "response_gen_f.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tnxXKDjq3jEL",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "from os import path\n",
        "import io\n",
        "import time\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, GRU,LSTM, Multiply\n",
        "from keras.layers import RepeatVector,TimeDistributed, Dense, Activation, Lambda,BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model, Model\n",
        "import keras.backend as K\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2_dVxfRQ5Gd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "wget http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/mono/OpenSubtitles.raw.en.gz\n",
        "mkdir /tmp/data/\n",
        "gunzip -c download.php?f=OpenSubtitles%2Fv2018%2Fmono%2FOpenSubtitles.raw.en.gz > /tmp/lines\n",
        "split -a 3 -l 100000  /tmp/lines /tmp/data/lines-"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy-ptvqRYQII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.listdir('/tmp/data/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roela1BK5Q7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "  w = w.strip()\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n11l-GSD26Mq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def line_cleaner(line):\n",
        "  #line =\"\"+str(line)\n",
        "  line=line.decode(\"utf-8\")\n",
        "  #print(type(line))\n",
        "  x=line.strip(\" -.!?\\n\")\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs9FGeQF2-2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines=[]\n",
        "i=0\n",
        "f= open('/tmp/data/lines-fli','rb')\n",
        "for line in f:\n",
        "  #print(line)\n",
        "  line=line_cleaner(line)\n",
        "  i=i+1\n",
        "  if(i>=10000):\n",
        "    break\n",
        "  lines.append(line)\n",
        "print(i,lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lrr8m4T2_gc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, validation_data = train_test_split(lines, test_size=.1, random_state=1234)\n",
        "print(len(train_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDEOnTr7BoC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Tx=20\n",
        "vocab_size=10000\n",
        "padding='post'\n",
        "trunc_type='post'\n",
        "oov_tok='<OOV>'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlasPhyZBzbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#vecs\n",
        "tokenizer=Tokenizer(num_words=vocab_size,oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(lines)\n",
        "word_index=tokenizer.word_index\n",
        "sequences=tokenizer.texts_to_sequences(lines)\n",
        "padded=pad_sequences(sequences,maxlen=Tx,padding=padding,truncating=trunc_type)\n",
        "print(padded[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fWwdTsvBVmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #MODEL 2 tokenizer\n",
        "# # Here I will use Tokenizer to extract the keyword vector as baseline\n",
        "# # I will use train data to fit the Tokenizer, then use this Tokenizer to extract the validation data\n",
        "# max_length = 100\n",
        "# max_features = 50000\n",
        "# token = Tokenizer(num_words=max_features)\n",
        "# token.fit_on_texts(list(np.asarray(train_data.question_text)))\n",
        "# xtrain = token.texts_to_sequences(np.asarray(train_data.question_text))\n",
        "# xvalidate = token.texts_to_sequences(np.asarray(validation_data.question_text))\n",
        "# xtest = token.texts_to_sequences(np.asarray(test.question_text))\n",
        "\n",
        "# # Because Tokenizer will split the sentence, for some sentence are smaller,\n",
        "# # so we have to pad the missing position\n",
        "# xtrain = pad_sequences(xtrain, maxlen=max_length)\n",
        "# xvalidate = pad_sequences(xvalidate, maxlen=max_length)\n",
        "# xtest = pad_sequences(xtest, maxlen=max_length)\n",
        "\n",
        "# ytrain = train_data.target\n",
        "# yvaliate = validation_data.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjbBWjgRBdHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(word_index)\n",
        "print(len(word_index))\n",
        "reverse_word_index={}\n",
        "for word,ind in word_index.items():\n",
        "  reverse_word_index[ind]=word\n",
        "print(reverse_word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HzvedN9CEB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contexts=np.array([padded[i] for i in range(0,len(lines)-1)])\n",
        "responses=np.array([padded[i] for i in range(1,len(lines))])\n",
        "responses=np.expand_dims(responses,-1)\n",
        "print(contexts[6])\n",
        "print(responses[6])\n",
        "print(contexts.shape,responses.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWmOkmaAC3gg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Cwp2qNlJP6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = \"/content/glove.6B.zip\"\n",
        "\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Unzipped glove')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH4sZOQSGwHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open( 'glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Number of word vectors is %s.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZx4-iKjC1qw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_dim=100\n",
        "num_words=vocab_size\n",
        "embedding_matrix=np.zeros((len(word_index)+1,embed_dim))\n",
        "for word,i in word_index.items():\n",
        "  embedding_vector=embeddings_index.get(word)\n",
        "  if(embedding_vector is not None and i <num_words):\n",
        "    embedding_matrix[i]=embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPGTPn9hIrR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Embedding\n",
        "embedding_layer=Embedding(len(word_index)+1,embed_dim,weights=[embedding_matrix],input_length=Tx,trainable=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnZwy1eMqChe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from faker import Faker\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from babel.dates import format_date\n",
        "#from nmt_utils import *\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KhPFBHiq1_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x, axis=1):\n",
        "    \"\"\"Softmax activation function.\n",
        "    # Arguments\n",
        "        x : Tensor.\n",
        "        axis: Integer, axis along which the softmax normalization is applied.\n",
        "    # Returns\n",
        "        Tensor, output of softmax transformation.\n",
        "    # Raises\n",
        "        ValueError: In case `dim(x) == 1`.\n",
        "    \"\"\"\n",
        "    ndim = K.ndim(x)\n",
        "    if ndim == 2:\n",
        "        return K.softmax(x)\n",
        "    elif ndim > 2:\n",
        "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
        "        s = K.sum(e, axis=axis, keepdims=True)\n",
        "        return e / s\n",
        "    else:\n",
        "        raise ValueError('Cannot apply softmax to a tensor that is 1D')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YysfyNaGqCeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Defined shared layers as global variables\n",
        "# repeator = RepeatVector(Tx)\n",
        "# concatenator = Concatenate(axis=-1)\n",
        "# densor1 = Dense(10, activation = \"tanh\")\n",
        "# densor2 = Dense(1, activation = \"relu\")\n",
        "# activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
        "# dotor = Dot(axes = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvmaWeuBr93s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# n_a = 128 # number of units for the pre-attention, bi-directional LSTM's hidden state 'a'\n",
        "# n_s = 128 # number of units for the post-attention LSTM's hidden state \"s\"\n",
        "\n",
        "# # Please note, this is the post attention LSTM cell.  \n",
        "# # For the purposes of passing the automatic grader\n",
        "# # please do not modify this global variable.  This will be corrected once the automatic grader is also updated.\n",
        "# post_activation_LSTM_cell = LSTM(n_s, return_state = True) # post-attention LSTM \n",
        "# output_layer = Dense(10000, activation=softmax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXgKqzMHqCXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # GRADED FUNCTION: one_step_attention\n",
        "\n",
        "# def one_step_attention(a, s_prev):\n",
        "#     \"\"\"\n",
        "#     Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
        "#     \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
        "    \n",
        "#     Arguments:\n",
        "#     a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
        "#     s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
        "    \n",
        "#     Returns:\n",
        "#     context -- context vector, input of the next (post-attention) LSTM cell\n",
        "#     \"\"\"\n",
        "    \n",
        "#     ### START CODE HERE ###\n",
        "#     # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
        "#     s_prev = repeator(s_prev)\n",
        "#     # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
        "#     # For grading purposes, please list 'a' first and 's_prev' second, in this order.\n",
        "#     concat = concatenator([a,s_prev])\n",
        "#     # Use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e. (≈1 lines)\n",
        "#     e = densor1(concat)\n",
        "#     # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies. (≈1 lines)\n",
        "#     energies = densor2(e)\n",
        "#     # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\" (≈ 1 line)\n",
        "#     alphas = activator(energies)\n",
        "#     # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
        "#     context = dotor([alphas,a])\n",
        "#     ### END CODE HERE ###\n",
        "    \n",
        "#     return context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q98Ds_eRqB9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
        "#     \"\"\"\n",
        "#     Arguments:\n",
        "#     Tx -- length of the input sequence\n",
        "#     Ty -- length of the output sequence\n",
        "#     n_a -- hidden state size of the Bi-LSTM\n",
        "#     n_s -- hidden state size of the post-attention LSTM\n",
        "#     human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
        "#     machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
        "\n",
        "#     Returns:\n",
        "#     model -- Keras model instance\n",
        "#     \"\"\"\n",
        "    \n",
        "#     # Define the inputs of your model with a shape (Tx,)\n",
        "#     # Define s0 (initial hidden state) and c0 (initial cell state)\n",
        "#     # for the decoder LSTM with shape (n_s,)\n",
        "#     X = Input(shape=(Tx, human_vocab_size))\n",
        "#     s0 = Input(shape=(n_s,), name='s0')\n",
        "#     c0 = Input(shape=(n_s,), name='c0')\n",
        "#     s = s0\n",
        "#     c = c0\n",
        "    \n",
        "#     # Initialize empty list of outputs\n",
        "#     outputs = []\n",
        "    \n",
        "\n",
        "#     ### START CODE HERE ###\n",
        "    \n",
        "#     # Step 1: Define your pre-attention Bi-LSTM. (≈ 1 line)\n",
        "#     a = Bidirectional(LSTM(units=n_a, return_sequences=True))(X)\n",
        "    \n",
        "#     # Step 2: Iterate for Ty steps\n",
        "#     for t in range(Ty):\n",
        "    \n",
        "#         # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
        "#         context = one_step_attention(a,s)\n",
        "        \n",
        "#         # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
        "#         # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
        "#         s, _, c = post_activation_LSTM_cell(context, initial_state= [s,c])\n",
        "        \n",
        "#         # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
        "#         out = output_layer(inputs=s)\n",
        "        \n",
        "#         # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
        "#         outputs.append(out)\n",
        "    \n",
        "#     # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
        "#     model = Model(inputs=[X,s0,c0], outputs=outputs)\n",
        "    \n",
        "#     ### END CODE HERE ###\n",
        "    \n",
        "#     return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hYI-zsYrKrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = model(Tx, Tx, n_a, n_s, 10000, 10000)\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb4gsGsJrKYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# opt = Adam(lr = 0.005,beta_1 =  0.9,beta_2 =  0.999)\n",
        "# model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9qV8iTdtAyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# s0=np.zeros((100,n_s))\n",
        "# c0=np.zeros((100,n_s))\n",
        "# model.fit([contexts], responses, epochs=20, batch_size=100)\n",
        "#doesnot work due to a dimensionerror."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEWMbow6I8Yo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(BatchNormalization())\n",
        "model.add(LSTM(units=256,return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(vocab_size,activation='softmax')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POCTdVzGKNu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SahDFxOkM3d5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(contexts,responses,epochs=10,batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRdZwrcsNHOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quest=[\"what are you doing\"]\n",
        "seq=tokenizer.texts_to_sequences(quest)\n",
        "pad=pad_sequences(seq,maxlen=Tx,padding=padding,truncating=trunc_type)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ulqOUjoYyed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred=model.predict(pad)\n",
        "pred=np.argmax(pred,axis=-1)\n",
        "print(pred,pred.shape)\n",
        "pred=pred.reshape(20,)\n",
        "for i in pred:\n",
        "  if i==0:\n",
        "    continue\n",
        "  else:\n",
        "    print(reverse_word_index[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_yXo39_Y2uy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "def beam_search_decoder(prediction, BW = 3):\n",
        "\n",
        "    output_sequences = [([], 0)]\n",
        "    \n",
        "    for sent in prediction:\n",
        "        new_sequences = []\n",
        "        sent = sent.reshape(20, )\n",
        "        print(sent.shape)\n",
        "        for old_seq, old_score in output_sequences:\n",
        "            for char_index in range(len(sent)):\n",
        "                new_seq = old_seq + [char_index]\n",
        "                if(sent[char_index]==0):\n",
        "                  continue\n",
        "                else:\n",
        "                  new_score = old_score + math.log(sent[char_index])\n",
        "                new_sequences.append((new_seq, new_score))\n",
        "                \n",
        "        output_sequences = sorted(new_sequences, key = lambda val: val[1], reverse = True)\n",
        "        output_sequences = output_sequences[:BW]\n",
        "        \n",
        "    return output_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCBx-fJmY-Yy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}